# Evaluaci√≥n de Modelos Ollama para Reemplazo de GitHub Models

**Fecha:** 2025
**Objetivo:** Identificar modelo local en Ollama que soporte tool calling para reemplazar GitHub Models (gpt-4o)

---

## üéØ RESUMEN EJECUTIVO

### ‚úÖ MODELO RECOMENDADO: **Mistral 7B v0.3**

**Raz√≥n:** √önico modelo verificado con soporte expl√≠cito de function calling (tool calling) en Ollama.

---

## üìã CRITERIOS DE EVALUACI√ìN

### Requisitos Cr√≠ticos (No Negociables)

1. **Tool Calling Support**: ‚úÖ OBLIGATORIO

   - Sistema usa LangChain con decoradores `@tool`
   - Agents 1-2 requieren `search_recent_papers` y `scrape_website`
   - Sin tool calling: outputs caen a 79-95 caracteres (probado en v2.0)

2. **Contexto M√≠nimo**: 32K tokens

   - 15 papers = ~19K tokens en resultados
   - Necesario para system prompts + razonamiento

3. **Compatibilidad LangChain**:

   - Debe funcionar con `ChatOllama` o `Ollama`
   - API compatible con OpenAI

4. **Tama√±o del Modelo**: ‚â§ 8B par√°metros
   - PC no soporta modelos grandes
   - M√°ximo ~5GB en disco

---

## üîç AN√ÅLISIS DE MODELOS DISPONIBLES

### ‚úÖ TIER 1: CONFIRMADO FUNCIONAL

#### **Mistral 7B v0.3** ‚≠ê RECOMENDADO

```
‚úÖ Tool Calling: S√ç (Expl√≠citamente documentado)
‚úÖ Context: 32K tokens
‚úÖ Tama√±o: 4.4GB
‚úÖ LangChain: Soportado con ChatOllama + bind_tools()
‚úÖ Documentaci√≥n: Ejemplos de function calling en raw mode
```

**Evidencia:**

- Documentaci√≥n oficial: "Mistral 0.3 supports function calling"
- Formato: `[AVAILABLE_TOOLS]` con estructura OpenAI-compatible
- Ejemplo documentado:
  ```json
  [AVAILABLE_TOOLS] [{"type": "function", "function": {...}}][/AVAILABLE_TOOLS]
  [INST] What is the weather like today in San Francisco [/INST]
  [TOOL_CALLS] [{"name": "get_current_weather", "arguments": {...}}]
  ```

**Comando de instalaci√≥n:**

```bash
ollama pull mistral:7b
```

**Integraci√≥n con LangChain:**

```python
from langchain_ollama import ChatOllama
from langchain.tools import tool

@tool
def search_recent_papers(query: str, max_results: int = 15):
    """Search Semantic Scholar for recent papers"""
    pass

llm = ChatOllama(
    model="mistral:7b",
    temperature=0
).bind_tools([search_recent_papers])
```

---

### ‚ùì TIER 2: POSIBLEMENTE FUNCIONAL (Requiere Pruebas)

#### **Qwen2.5 8B**

```
‚ùì Tool Calling: NO CONFIRMADO (etiqueta "tools" pero sin documentaci√≥n expl√≠cita)
‚úÖ Context: 32K tokens (128K en versiones m√°s grandes)
‚úÖ Tama√±o: 4.7GB
‚úÖ JSON: Generaci√≥n estructurada mejorada (mencionado en docs)
‚ö†Ô∏è  Necesita verificaci√≥n: GitHub repo o comunidad LangChain
```

**Estado:**

- Tag "tools" visible en Ollama Library
- Fuerte capacidad de JSON generation
- Sin ejemplos expl√≠citos de function calling
- Qwen3 menciona "tool use" en documentaci√≥n general
- **Acci√≥n requerida**: Probar con LangChain `bind_tools()`

**Comando de instalaci√≥n:**

```bash
ollama pull qwen2.5:8b
```

#### **Phi3 3.8B**

```
‚ùå Tool Calling: NO MENCIONADO en documentaci√≥n
‚úÖ‚úÖ Context: 128K tokens (excelente!)
‚úÖ Tama√±o: 2.2GB (muy eficiente)
‚úÖ Capacidades: Razonamiento, matem√°ticas, c√≥digo
‚ö†Ô∏è  Foco: Instruction following, no tool use
```

**Estado:**

- Documentaci√≥n NO menciona function calling
- Enfoque en razonamiento y contexto largo
- Probable que NO soporte tool calling nativamente
- **Recomendaci√≥n**: Solo si Mistral y Qwen fallan

---

### ‚õî TIER 3: NO RECOMENDADOS

#### **Gemma2 4B**

```
‚ùå Tool Calling: NO MENCIONADO
‚ö†Ô∏è  Context: Solo 8K tokens (insuficiente para 15 papers)
‚úÖ Tama√±o: 1.6GB (versi√≥n 2b)
‚ùå Documentaci√≥n: Solo casos de uso generales
```

**Raz√≥n de descarte:** Contexto insuficiente + sin tool calling

#### **Zephyr 7B**

```
‚ùì Tool Calling: NO MENCIONADO
‚úÖ Context: 32K tokens
‚úÖ Tama√±o: 4.1GB
‚ÑπÔ∏è  Nota: Basado en Mistral (fine-tuned)
```

**Raz√≥n de descarte:** Sin evidencia de tool calling (aunque base es Mistral)

#### **DeepSeek-Coder 6.7B**

```
‚ùå Tool Calling: NO MENCIONADO
‚úÖ Context: 16K tokens (l√≠mite)
‚úÖ Tama√±o: 3.8GB
‚ö†Ô∏è  Enfoque: Code generation (no tool use)
```

**Raz√≥n de descarte:** Especializado en c√≥digo, no tool calling

#### **CodeGemma 7B**

```
‚ùå Tool Calling: NO INVESTIGADO (probablemente no)
‚ùì Context: Desconocido
‚ö†Ô∏è  Enfoque: Code generation
```

**Raz√≥n de descarte:** Similar a DeepSeek-Coder

#### **Nomic-Embed-Text**

```
‚ùå No aplicable: Modelo de embeddings
```

#### **Llava 7B**

```
‚ùå No aplicable: Modelo de visi√≥n (multimodal)
```

---

## üîß INTEGRACI√ìN CON LANGCHAIN

### Documentaci√≥n Oficial Verificada

**Fuente:** https://python.langchain.com/docs/integrations/chat/ollama

**Soporte de Tool Calling:**

```python
# ChatOllama soporta bind_tools() con modelos compatibles
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="mistral:7b",  # Modelo con tool support
    temperature=0
).bind_tools([validate_user])  # M√©todo est√°ndar de LangChain

result = llm.invoke("Could you validate user 123?")
# Output: result.tool_calls contiene llamadas estructuradas
```

**Requisitos:**

- `langchain-ollama` package instalado
- Ollama server corriendo (`ollama serve`)
- Modelo debe estar en lista de [modelos con tool support](https://ollama.com/search?&c=tools)

---

## üìä COMPARACI√ìN CON GITHUB MODELS

| Caracter√≠stica     | GitHub Models (gpt-4o) | Mistral 7B v0.3 (Ollama) |
| ------------------ | ---------------------- | ------------------------ |
| **Tool Calling**   | ‚úÖ Completo            | ‚úÖ Soportado (raw mode)  |
| **Context**        | 128K tokens            | 32K tokens               |
| **L√≠mite Request** | 8K tokens body         | Sin l√≠mite local         |
| **Rate Limit**     | 50 req/day             | ‚àû (local)                |
| **Costo**          | $0.00 (beta)           | $0.00 (local)            |
| **Velocidad**      | API (variable)         | Local (depende GPU)      |
| **Disponibilidad** | Requiere internet      | Offline                  |

**Ventajas de Mistral Local:**

- ‚úÖ Sin rate limits (desarrollo ilimitado)
- ‚úÖ Sin dependencia de internet
- ‚úÖ Sin costos futuros (post-beta)

**Desventajas:**

- ‚ö†Ô∏è Contexto menor (32K vs 128K)
- ‚ö†Ô∏è Requiere hardware adecuado
- ‚ö†Ô∏è Posible reducci√≥n en calidad de outputs vs gpt-4o

---

## üß™ PLAN DE PRUEBAS RECOMENDADO

### Fase 1: Verificaci√≥n B√°sica de Tool Calling (30 min)

```python
# test_mistral_tools.py
from langchain_ollama import ChatOllama
from langchain_core.tools import tool

@tool("search_test")
def search_test(query: str, max_results: int = 5) -> str:
    """Test function for tool calling verification"""
    return f"Searched for: {query}, max: {max_results}"

# Configurar Mistral
llm = ChatOllama(
    model="mistral:7b",
    temperature=0
).bind_tools([search_test])

# Test 1: Verificar que reconoce la herramienta
result = llm.invoke("Search for 'deep learning' papers, max 10 results")

# Verificaciones
assert hasattr(result, 'tool_calls'), "No tool_calls attribute"
assert len(result.tool_calls) > 0, "No tool calls generated"
assert result.tool_calls[0]['name'] == 'search_test', "Wrong tool called"
print("‚úÖ Tool calling funciona correctamente")
print(f"Tool calls: {result.tool_calls}")
```

### Fase 2: Prueba con Tools Reales (1 hora)

```python
# test_agent1_mistral.py
from graphs.research_graph import create_research_graph
from langchain_ollama import ChatOllama

# Modificar config temporalmente
test_llm = ChatOllama(
    model="mistral:7b",
    temperature=0,
    num_ctx=32768  # Contexto completo
)

# Probar con Agent 1 (Niche Analyst)
# - Usar query simple: "deep learning"
# - Max 5 papers (para prueba r√°pida)
# - Verificar que:
#   1. Llama search_recent_papers correctamente
#   2. Procesa resultados
#   3. Genera an√°lisis coherente (>500 caracteres)
```

### Fase 3: Comparaci√≥n de Calidad (2 horas)

**Escenarios de prueba:**

1. **Query simple:** "machine learning"

   - Comparar outputs gpt-4o vs mistral:7b
   - M√©tricas: longitud, coherencia, uso de herramientas

2. **Query compleja:** "transformer architectures for NLP"

   - Verificar manejo de contexto
   - Validar scraping web funciona

3. **Contexto largo:** 15 papers
   - Verificar que no excede 32K tokens
   - Medir degradaci√≥n vs 5 papers

### Fase 4: Prueba End-to-End (3 horas)

```python
# Ejecutar workflow completo con Mistral
# Agents 1-5, query: "reinforcement learning"
# Documentar:
# - Tiempo total de ejecuci√≥n
# - Calidad de outputs por agente
# - Errores/warnings
# - Tokens consumidos por agente
```

---

## üìù RECOMENDACIONES FINALES

### Opci√≥n A: Usar Mistral 7B (RECOMENDADA) ‚úÖ

**Cu√°ndo:**

- Desarrollo intensivo (muchas pruebas)
- GitHub Models rate limit alcanzado
- Necesitas trabajo offline

**Pasos:**

1. `ollama pull mistral:7b`
2. Ejecutar Fase 1 de pruebas (verificaci√≥n b√°sica)
3. Si funciona: Modificar `config/settings.py` para agregar opci√≥n Ollama
4. Si falla: Pasar a Opci√≥n B

### Opci√≥n B: Probar Qwen2.5 (ALTERNATIVA) ‚ùì

**Cu√°ndo:**

- Mistral no cumple calidad esperada
- Necesitas mejor contexto/JSON handling

**Pasos:**

1. `ollama pull qwen2.5:8b`
2. Ejecutar misma Fase 1
3. Si no hay tool_calls: Investigar implementaci√≥n custom

### Opci√≥n C: Mantener GitHub Models (FALLBACK) ‚ö†Ô∏è

**Cu√°ndo:**

- Ning√∫n modelo local funciona con tool calling
- Calidad es cr√≠tica (gpt-4o superior)
- Rate limit manejable (50 req/day suficiente)

**Mejoras:**

- Implementar cache de respuestas
- Agregar delay entre requests
- Usar solo para pruebas finales

### Opci√≥n D: Modelo M√°s Grande (√öLTIMA OPCI√ìN) üîß

**Si PC lo permite:**

- `qwen2.5:14b` (9.0GB) - Mejor capacidad, verificar tool support
- `mistral:7b-instruct-v0.3` - Versi√≥n instruct espec√≠fica

---

## üöÄ PR√ìXIMOS PASOS INMEDIATOS

1. **[5 min]** Instalar Mistral:

   ```bash
   ollama pull mistral:7b
   ollama run mistral:7b  # Verificar descarga
   ```

2. **[10 min]** Crear script de prueba b√°sico (Fase 1)

3. **[15 min]** Ejecutar y documentar resultados

4. **[Decisi√≥n]**
   - ‚úÖ Si funciona ‚Üí Proceder Fase 2 (integraci√≥n real)
   - ‚ùå Si falla ‚Üí Probar Qwen2.5 o reportar hallazgos

---

## üìö REFERENCIAS

### Documentaci√≥n Verificada

1. **Mistral Tool Calling:** https://ollama.com/library/mistral
   - Secci√≥n "Function calling with Mistral 0.3"
2. **LangChain + Ollama:** https://python.langchain.com/docs/integrations/chat/ollama
   - Secci√≥n "Tool calling"
3. **Qwen3 Tool Use:** https://github.com/QwenLM/Qwen3

   - Secci√≥n "Build with Qwen3 > Tool Use"

4. **Ollama Tool Support:** https://ollama.com/search?&c=tools
   - Lista de modelos con tag "tools"

### Modelos con Tool Support Confirmado (Ollama)

- ‚úÖ mistral (v0.3+)
- ‚úÖ gpt-oss:20b (mencionado en docs LangChain)
- ‚ùì qwen2.5 (tag presente, sin confirmar)
- ‚ùì llama3.1 (mencionado en algunos contextos)

---

## ‚ö†Ô∏è ADVERTENCIAS

1. **Tool Calling != Code Generation**

   - Modelos de c√≥digo (DeepSeek, CodeGemma) NO son para tool calling
   - Son para completion/generation, no function orchestration

2. **Contexto es Cr√≠tico**

   - 15 papers + system prompts ‚âà 20-25K tokens
   - Modelos con <32K no son viables
   - Mistral 32K es justo el m√≠nimo

3. **Quality vs Speed Tradeoff**

   - gpt-4o (GitHub Models) probablemente tiene mejor calidad
   - Mistral 7B ser√° m√°s r√°pido pero posiblemente menos preciso
   - Evaluar si tradeoff es aceptable para tu caso de uso

4. **Hardware Requirements**
   - Mistral 7B (4.4GB) requiere ~8GB RAM
   - GPU recomendada para velocidad decente
   - Sin GPU: expect respuestas lentas (30-60s por request)

---

**√öltima actualizaci√≥n:** 2025-01-XX
**Estado:** Investigaci√≥n completada, pendiente pruebas pr√°cticas
**Decisi√≥n recomendada:** Probar Mistral 7B v0.3 inmediatamente
