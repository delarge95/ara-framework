# üìö Gu√≠a Completa de LangGraph para ARA Framework

## √çndice

1. [¬øQu√© es LangGraph?](#qu√©-es-langgraph)
2. [Conceptos Fundamentales](#conceptos-fundamentales)
3. [Arquitectura del ARA Framework](#arquitectura-del-ara-framework)
4. [Paso a Paso del Workflow](#paso-a-paso-del-workflow)
5. [Implementaci√≥n Pr√°ctica](#implementaci√≥n-pr√°ctica)
6. [Visualizaci√≥n de Grafos](#visualizaci√≥n-de-grafos)
7. [Comparaci√≥n con CrewAI](#comparaci√≥n-con-crewai)
8. [Casos de Uso Avanzados](#casos-de-uso-avanzados)

---

## ¬øQu√© es LangGraph?

**LangGraph** es un framework de orquestaci√≥n de agentes de LangChain que permite construir aplicaciones multi-agente complejas como **grafos de estado** (state graphs). A diferencia de pipelines lineales o DAGs est√°ticos, LangGraph permite:

### üéØ Caracter√≠sticas Principales

1. **Flujo expl√≠cito y controlado**: Defines exactamente c√≥mo fluyen los datos entre nodos
2. **Estado centralizado**: Un √∫nico objeto de estado compartido entre todos los agentes
3. **Checkpointing**: Pausa y reanuda workflows en cualquier punto
4. **Condicionales y loops**: Agrega l√≥gica compleja de enrutamiento
5. **Observabilidad completa**: Integraci√≥n nativa con LangSmith
6. **Production-ready**: Usado por Uber, LinkedIn, Replit, Elastic

### üÜö ¬øPor qu√© LangGraph vs. otros frameworks?

| Caracter√≠stica    | LangGraph    | CrewAI       | LangChain LCEL |
| ----------------- | ------------ | ------------ | -------------- |
| Control de flujo  | ‚úÖ Expl√≠cito | ‚ö†Ô∏è Impl√≠cito | ‚úÖ Expl√≠cito   |
| Checkpointing     | ‚úÖ Nativo    | ‚ùå No        | ‚ùå No          |
| Estado compartido | ‚úÖ TypedDict | ‚ö†Ô∏è Limitado  | ‚ùå No          |
| Condicionales     | ‚úÖ Full      | ‚ö†Ô∏è Limitado  | ‚úÖ S√≠          |
| Loops/Ciclos      | ‚úÖ S√≠        | ‚ùå No        | ‚ö†Ô∏è Complejo    |
| Debugging         | ‚úÖ Excelente | ‚ö†Ô∏è Dif√≠cil   | ‚úÖ Bueno       |
| Escalabilidad     | ‚úÖ Alta      | ‚ö†Ô∏è Media     | ‚úÖ Alta        |

---

## Conceptos Fundamentales

### 1. StateGraph: El Contenedor Principal

```python
from langgraph.graph import StateGraph, END

# Define el esquema del estado
class ResearchState(TypedDict):
    niche: str
    niche_analysis: Optional[str]
    messages: Annotated[Sequence[BaseMessage], add]
    errors: list[str]

# Crea el grafo
workflow = StateGraph(ResearchState)
```

**StateGraph** es el contenedor que gestiona:

- **Nodos**: Funciones que procesan el estado
- **Edges**: Conexiones entre nodos (secuenciales o condicionales)
- **Estado**: Datos compartidos entre todos los nodos

### 2. Nodos (Nodes): Funciones que Transforman el Estado

Cada nodo es una funci√≥n que:

1. Recibe el estado actual
2. Realiza alguna operaci√≥n (llamar LLM, ejecutar herramienta, etc.)
3. Retorna un diccionario con actualizaciones al estado

```python
def niche_analyst_node(state: ResearchState) -> ResearchState:
    # 1. Inicializar LLM
    llm = ChatGroq(model="llama-3.3-70b-versatile")

    # 2. Configurar herramientas
    tools = [scraping_tool, search_tool]

    # 3. Crear agente ReAct
    agent = create_react_agent(llm, tools)

    # 4. Ejecutar agente
    result = agent.invoke({
        "messages": [
            SystemMessage(content="You are a niche analyst..."),
            HumanMessage(content=f"Analyze: {state['niche']}")
        ]
    })

    # 5. Actualizar estado
    return {
        **state,  # Copia todo el estado existente
        "niche_analysis": result["messages"][-1].content,  # Nueva salida
        "agent_history": state.get("agent_history", []) + ["niche_analyst"],
    }
```

### 3. Edges (Conexiones): C√≥mo Fluyen los Datos

#### üîπ Edges Secuenciales

Conexiones directas de un nodo a otro:

```python
workflow.add_edge("niche_analyst", "literature_researcher")
# Despu√©s de niche_analyst, siempre ejecuta literature_researcher
```

#### üîπ Edges Condicionales

Decisiones basadas en el estado:

```python
def should_retry_or_continue(state: ResearchState) -> str:
    if len(state.get("errors", [])) > 0:
        if state.get("retry_count", {}).get("current_agent", 0) < 3:
            return "retry"
        else:
            return "fail"
    return "continue"

workflow.add_conditional_edges(
    "niche_analyst",
    should_retry_or_continue,
    {
        "retry": "niche_analyst",  # Vuelve a ejecutar el mismo nodo
        "continue": "literature_researcher",  # Contin√∫a al siguiente
        "fail": END,  # Termina el workflow
    }
)
```

### 4. Estado (State): El Coraz√≥n del Grafo

El estado es un **TypedDict** que define:

- **Campos obligatorios** (sin Optional)
- **Campos opcionales** (con Optional[T])
- **Acumuladores** (con Annotated[List[T], add])

```python
from typing import TypedDict, Optional, Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph import add

class ResearchState(TypedDict):
    # INPUT (obligatorio)
    niche: str

    # OUTPUTS (opcionales, se llenan progresivamente)
    niche_analysis: Optional[str]
    literature_review: Optional[str]
    technical_architecture: Optional[str]
    implementation_plan: Optional[str]
    final_report: Optional[str]

    # ACUMULADORES (agregan elementos sin sobrescribir)
    messages: Annotated[Sequence[BaseMessage], add]

    # METADATA (tracking de ejecuci√≥n)
    current_agent: str
    agent_history: list[str]
    start_time: str
    end_time: Optional[str]

    # ERROR HANDLING
    errors: list[str]
    warnings: list[str]
    retry_count: dict[str, int]

    # BUDGET TRACKING
    total_credits_used: float
    budget_limit: float
    budget_exceeded: bool
```

**Nota sobre Acumuladores**: El operador `add` en `Annotated[Sequence[BaseMessage], add]` indica que cada nodo **agrega** mensajes en lugar de sobrescribirlos:

```python
# Sin add (sobrescribe):
state["messages"] = [new_message]  # Pierde mensajes anteriores

# Con add (acumula):
state["messages"] = state["messages"] + [new_message]  # Mantiene historial
```

### 5. Checkpointing: Pausa y Reanuda

El checkpointing permite:

- ‚úÖ Guardar el estado en cualquier nodo
- ‚úÖ Reanudar ejecuci√≥n despu√©s de un crash
- ‚úÖ Implementar "human-in-the-loop"
- ‚úÖ Debugging paso a paso

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.redis import RedisSaver

# Opci√≥n 1: En memoria (desarrollo)
checkpointer = MemorySaver()

# Opci√≥n 2: Redis (producci√≥n)
checkpointer = RedisSaver(host="localhost", port=6379)

# Compilar con checkpointing
graph = workflow.compile(checkpointer=checkpointer)

# Ejecutar con thread_id (identifica la sesi√≥n)
config = {"configurable": {"thread_id": "research-001"}}
result = await graph.ainvoke(initial_state, config=config)

# Reanudar m√°s tarde con el mismo thread_id
continued = await graph.ainvoke({}, config=config)  # Contin√∫a desde donde qued√≥
```

---

## Arquitectura del ARA Framework

### üèóÔ∏è Visi√≥n General

El ARA Framework implementa un **pipeline secuencial de 5 agentes** que trabajan en cadena para generar an√°lisis de investigaci√≥n acad√©mica:

```
START ‚Üí Niche Analyst ‚Üí Literature Researcher ‚Üí Technical Architect
      ‚Üí Implementation Specialist ‚Üí Content Synthesizer ‚Üí END
```

### üìä Flujo de Estado

```
Initial State:
{
    "niche": "Rust WebAssembly for real-time audio",
    "messages": [],
    "agent_history": [],
    ...
}

‚Üì Niche Analyst (7-8 min)

{
    "niche": "Rust WebAssembly...",
    "niche_analysis": "# An√°lisis de Viabilidad\n...",  ‚Üê AGREGADO
    "agent_history": ["niche_analyst"],  ‚Üê ACTUALIZADO
    "messages": [SystemMessage, HumanMessage, AIMessage],  ‚Üê ACUMULADO
    ...
}

‚Üì Literature Researcher (20-25 min)

{
    ...
    "niche_analysis": "...",  ‚Üê PRESERVADO
    "literature_review": "# Revisi√≥n de Literatura\n...",  ‚Üê AGREGADO
    "agent_history": ["niche_analyst", "literature_researcher"],  ‚Üê ACTUALIZADO
    ...
}

... (contin√∫a con los 3 agentes restantes)

‚Üì Content Synthesizer (15-18 min)

Final State:
{
    "niche": "Rust WebAssembly...",
    "niche_analysis": "...",  ‚Üê DISPONIBLE
    "literature_review": "...",  ‚Üê DISPONIBLE
    "technical_architecture": "...",  ‚Üê DISPONIBLE
    "implementation_plan": "...",  ‚Üê DISPONIBLE
    "final_report": "# Reporte Final (10K-15K palabras)\n...",  ‚Üê FINAL OUTPUT
    "agent_history": ["niche_analyst", "literature_researcher", ...],
    "messages": [500+ mensajes de toda la conversaci√≥n],
    "end_time": "2025-11-11T18:30:00Z",
    ...
}
```

### üéØ Los 5 Agentes

#### 1Ô∏è‚É£ **Niche Analyst** (Analista de Nicho)

- **Duraci√≥n**: ~7-8 minutos
- **Costo**: $0 (Groq es gratis)
- **Herramientas**:
  - `scraping_tool`: Scrape GitHub, Reddit, blogs
  - `search_tool`: Busca papers en Semantic Scholar
- **Entrada**: `niche` (tema de investigaci√≥n)
- **Salida**: `niche_analysis` (viabilidad score, tendencias, keywords, sub-nichos)

```python
def niche_analyst_node(state: ResearchState) -> ResearchState:
    llm = ChatGroq(model="llama-3.3-70b-versatile")
    tools = [scraping.scrape_website, search.search_recent_papers]
    agent = create_react_agent(llm, tools)

    result = agent.invoke({
        "messages": [
            SystemMessage(content="""You are an expert Niche Market Analyst with 10+ years...

            REQUIREMENTS:
            1. Academic Research (Semantic Scholar):
               - Search for papers related to the niche
               - Analyze publication trends (last 3 years)
               - Identify key researchers and institutions

            2. Community Activity:
               - GitHub projects (stars, forks, recent commits)
               - Reddit discussions and sentiment
               - Blog posts and technical articles

            3. Market Evaluation:
               - Viability score (0-100)
               - Growth trends
               - Competition level

            OUTPUT FORMAT:
            ## Viability Score: [0-100]

            ## Trends and Opportunities
            - ...

            ## Key Academic Papers
            - ...

            ## Community Insights
            - ...

            IMPORTANT: Respect Semantic Scholar rate limit (1 request/sec)
            """),
            HumanMessage(content=f"Analyze niche: {state['niche']}")
        ]
    })

    return {
        **state,
        "niche_analysis": result["messages"][-1].content,
        "current_agent": "literature_researcher",
        "agent_history": state.get("agent_history", []) + ["niche_analyst"],
        "messages": state.get("messages", []) + result["messages"],
    }
```

#### 2Ô∏è‚É£ **Literature Researcher** (Investigador de Literatura)

- **Duraci√≥n**: ~20-25 minutos
- **Costo**: ~$0.15-1.50 (mostly rate limits)
- **Herramientas**:
  - `search_tool`: Semantic Scholar API
  - `pdf_tool`: Extrae contenido de PDFs
  - `database_tool`: Guarda en Supabase
- **Entrada**: `niche_analysis` (primeros 2000 chars)
- **Salida**: `literature_review` (revisi√≥n de literatura, bibliograf√≠a de 20 papers)

**Contexto pasado del agente anterior**:

```python
context = f"""
PREVIOUS NICHE ANALYSIS (first 2000 chars):
{state.get('niche_analysis', '')[:2000]}
"""
```

#### 3Ô∏è‚É£ **Technical Architect** (Arquitecto T√©cnico)

- **Duraci√≥n**: ~10-12 minutos
- **Costo**: ~$1.00
- **Herramientas**: scraping, pdf, database
- **Entrada**: `niche_analysis` (1500 chars) + `literature_review` (3000 chars)
- **Salida**: `technical_architecture` (arquitectura de sistema, stack tecnol√≥gico, patrones)

#### 4Ô∏è‚É£ **Implementation Specialist** (Especialista en Implementaci√≥n)

- **Duraci√≥n**: ~7-8 minutos
- **Costo**: ~$0.33
- **Herramientas**: scraping, database
- **Entrada**: `technical_architecture` (3000 chars)
- **Salida**: `implementation_plan` (roadmap, sprints, user stories, timeline)

#### 5Ô∏è‚É£ **Content Synthesizer** (Sintetizador de Contenido)

- **Duraci√≥n**: ~15-18 minutos
- **Costo**: ~$0.50
- **Herramientas**: database
- **Entrada**: Todos los 4 outputs anteriores (truncados)
- **Salida**: `final_report` (reporte de 10K-15K palabras con executive summary, TOC, referencias)

**Integraci√≥n completa**:

```python
context = f"""
NICHE ANALYSIS (first 2000 chars):
{state.get('niche_analysis', '')[:2000]}

LITERATURE REVIEW (first 3000 chars):
{state.get('literature_review', '')[:3000]}

TECHNICAL ARCHITECTURE (first 3000 chars):
{state.get('technical_architecture', '')[:3000]}

IMPLEMENTATION PLAN (first 2000 chars):
{state.get('implementation_plan', '')[:2000]}
"""
```

### üìà M√©tricas del Pipeline

| M√©trica                | Valor                                     |
| ---------------------- | ----------------------------------------- |
| **Duraci√≥n Total**     | 60-75 minutos                             |
| **Costo Total**        | $0.10-0.20 por an√°lisis                   |
| **Tokens Procesados**  | ~200K-300K tokens                         |
| **LLM Calls**          | ~15-25 llamadas                           |
| **API Calls Externos** | ~200-300 (Semantic Scholar, GitHub, etc.) |
| **Output Final**       | 10K-15K palabras                          |
| **Precisi√≥n**          | 85-90% (seg√∫n evaluaciones manuales)      |

---

## Paso a Paso del Workflow

### üöÄ Ejecuci√≥n Completa

#### Paso 1: Crear el Grafo

```python
from graphs.research_graph import create_research_graph

graph = create_research_graph(
    enable_checkpointing=True,
    checkpoint_backend="memory"  # o "redis" para producci√≥n
)
```

Internamente, `create_research_graph()` hace:

```python
def create_research_graph(
    enable_checkpointing: bool = True,
    checkpoint_backend: Literal["memory", "redis"] = "memory",
) -> StateGraph:
    # 1. Inicializar StateGraph
    workflow = StateGraph(ResearchState)

    # 2. Agregar los 5 nodos
    workflow.add_node("niche_analyst", niche_analyst_node)
    workflow.add_node("literature_researcher", literature_researcher_node)
    workflow.add_node("technical_architect", technical_architect_node)
    workflow.add_node("implementation_specialist", implementation_specialist_node)
    workflow.add_node("content_synthesizer", content_synthesizer_node)

    # 3. Definir punto de entrada
    workflow.set_entry_point("niche_analyst")

    # 4. Conectar nodos con edges secuenciales
    workflow.add_edge("niche_analyst", "literature_researcher")
    workflow.add_edge("literature_researcher", "technical_architect")
    workflow.add_edge("technical_architect", "implementation_specialist")
    workflow.add_edge("implementation_specialist", "content_synthesizer")
    workflow.add_edge("content_synthesizer", END)

    # 5. Configurar checkpointing
    if enable_checkpointing:
        if checkpoint_backend == "memory":
            checkpointer = MemorySaver()
        else:
            checkpointer = RedisSaver(host="localhost", port=6379)
    else:
        checkpointer = None

    # 6. Compilar grafo
    return workflow.compile(checkpointer=checkpointer)
```

#### Paso 2: Inicializar el Estado

```python
from datetime import datetime, timezone

initial_state: ResearchState = {
    # INPUT
    "niche": "Rust WebAssembly for real-time audio processing",

    # OUTPUTS (vac√≠os al inicio)
    "niche_analysis": None,
    "literature_review": None,
    "technical_architecture": None,
    "implementation_plan": None,
    "final_report": None,

    # MESSAGES (vac√≠o, se acumular√°n)
    "messages": [],

    # METADATA
    "current_agent": "niche_analyst",
    "agent_history": [],
    "start_time": datetime.now(timezone.utc).isoformat(),
    "end_time": None,

    # ERROR HANDLING
    "errors": [],
    "warnings": [],
    "retry_count": {},

    # BUDGET
    "total_credits_used": 0.0,
    "budget_limit": 10.0,
    "budget_exceeded": False,
}
```

#### Paso 3: Ejecutar el Grafo

```python
# Opci√≥n A: Ejecuci√≥n s√≠ncrona (bloquea hasta terminar)
result = graph.invoke(initial_state)

# Opci√≥n B: Ejecuci√≥n as√≠ncrona (recomendada)
result = await graph.ainvoke(initial_state)

# Opci√≥n C: Con checkpointing y thread_id
config = {"configurable": {"thread_id": "research-001"}}
result = await graph.ainvoke(initial_state, config=config)
```

#### Paso 4: Acceder a los Resultados

```python
# Resultado final
print(result["final_report"])

# Outputs intermedios
print(result["niche_analysis"])
print(result["literature_review"])
print(result["technical_architecture"])
print(result["implementation_plan"])

# Metadata
print(f"Agentes ejecutados: {result['agent_history']}")
print(f"Errores: {result['errors']}")
print(f"Duraci√≥n: {result['start_time']} ‚Üí {result['end_time']}")
print(f"Cr√©ditos usados: ${result['total_credits_used']:.2f}")
print(f"Total de mensajes: {len(result['messages'])}")
```

### üîß Funci√≥n de Conveniencia

Para simplificar, el framework provee `run_research_pipeline()`:

```python
from graphs.research_graph import run_research_pipeline

# Una l√≠nea hace todo
result = await run_research_pipeline(
    niche="Rust WebAssembly for real-time audio processing",
    budget_limit=10.0,
    enable_checkpointing=True,
)

# Acceso directo a resultados
print(result["final_report"])
```

Internamente, esta funci√≥n:

1. Crea el grafo con `create_research_graph()`
2. Inicializa el estado completo
3. Ejecuta `graph.ainvoke()`
4. Retorna el estado final

---

## Implementaci√≥n Pr√°ctica

### üõ†Ô∏è Configuraci√≥n del Proyecto

#### 1. Instalar Dependencias

```bash
# Dependencias principales
pip install langgraph langchain langchain-core langchain-community
pip install langchain-groq groq

# Herramientas
pip install playwright beautifulsoup4 lxml
pip install semanticscholar arxiv pymupdf

# Base de datos y caching
pip install supabase redis hiredis

# Utilidades
pip install structlog rich typer
```

#### 2. Configurar Variables de Entorno

```bash
# .env
GROQ_API_KEY=gsk_...
SUPABASE_URL=https://...
SUPABASE_KEY=eyJ...
REDIS_URL=redis://localhost:6379
LANGSMITH_API_KEY=ls__...  # Opcional para observabilidad
```

#### 3. Estructura del Proyecto

```
ara_framework/
‚îú‚îÄ‚îÄ graphs/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ research_graph.py       # ‚Üê Implementaci√≥n del grafo
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scraping_tool.py        # ‚Üê Playwright scraping
‚îÇ   ‚îú‚îÄ‚îÄ search_tool.py          # ‚Üê Semantic Scholar
‚îÇ   ‚îú‚îÄ‚îÄ pdf_tool.py             # ‚Üê PyMuPDF extractor
‚îÇ   ‚îî‚îÄ‚îÄ database_tool.py        # ‚Üê Supabase persistence
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # ‚Üê Settings con pydantic
‚îÇ   ‚îî‚îÄ‚îÄ budget_manager.py       # ‚Üê Budget tracking
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_langgraph.py       # ‚Üê Tests del grafo
‚îú‚îÄ‚îÄ visualize_graph.py          # ‚Üê Visualizaci√≥n
‚îî‚îÄ‚îÄ requirements.txt
```

### üìù Ejemplo de Nodo Completo

```python
import structlog
from typing import Optional
from datetime import datetime, timezone
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.prebuilt import create_react_agent
from tools import scraping, search
from core.config import settings

logger = structlog.get_logger(__name__)

def niche_analyst_node(state: ResearchState) -> ResearchState:
    """
    Analiza la viabilidad de un nicho de investigaci√≥n.

    Responsabilidades:
    1. Buscar papers acad√©micos en Semantic Scholar
    2. Scrapear actividad en GitHub, Reddit, blogs
    3. Evaluar tendencias y oportunidades
    4. Calcular viability score (0-100)

    Tools:
    - scraping_tool: Scrape websites (GitHub, Reddit, etc.)
    - search_tool: Semantic Scholar API

    Duration: ~7-8 minutes
    Credits: 0 (Groq is free)
    """
    logger.info(
        "niche_analyst_started",
        agent_number=1,
        niche=state["niche"],
        timestamp=datetime.now(timezone.utc).isoformat(),
    )

    try:
        # 1. Inicializar LLM
        llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            temperature=0.7,
            api_key=settings.GROQ_API_KEY,
        )

        # 2. Obtener herramientas
        tools = [
            scraping.scrape_website,
            scraping.scrape_multiple_urls,
            search.search_recent_papers,
        ]

        # 3. Crear agente ReAct
        agent = create_react_agent(llm, tools)

        # 4. Construir prompts con contexto
        system_msg = SystemMessage(content=f"""You are an expert Niche Market Analyst with 10+ years of experience in technology research and market evaluation. You have a PhD in Computer Science and deep knowledge of academic research methodologies.

CONTEXT:
You are analyzing the niche: "{state['niche']}"

REQUIREMENTS:

1. Academic Research (Semantic Scholar):
   - Search for recent papers (last 3 years) related to the niche
   - Analyze publication trends: Are publications increasing or decreasing?
   - Identify key researchers, institutions, and research groups
   - Look for highly-cited papers (>50 citations)
   - Note: RESPECT 1 request/second rate limit

2. Community Activity (Web Scraping):
   - GitHub Projects:
     * Search for relevant repositories
     * Analyze stars, forks, recent commits (last 6 months)
     * Check issue activity and maintainer responsiveness
   - Reddit Discussions:
     * Search r/programming, r/MachineLearning, relevant subreddits
     * Analyze sentiment and engagement
   - Technical Blogs:
     * Search for blog posts and tutorials
     * Check publication frequency

3. Market Evaluation:
   - Calculate a viability score (0-100) based on:
     * Academic activity (30%): Publication trends, citation counts
     * Community engagement (30%): GitHub activity, Reddit discussions
     * Industry adoption (20%): Blog posts, company implementations
     * Growth potential (20%): Emerging trends, future applications
   - Identify sub-niches and related areas
   - List potential challenges and opportunities

OUTPUT FORMAT (Markdown):

## Viability Score: [0-100]
Brief justification (2-3 sentences)

## Academic Landscape
### Publication Trends
- [Trend description]
- Key papers: [Title 1], [Title 2], [Title 3]

### Key Researchers and Institutions
- [Researcher/Institution 1]
- [Researcher/Institution 2]

## Community Activity
### GitHub Projects
- [Project 1]: [stats and activity]
- [Project 2]: [stats and activity]

### Discussions and Sentiment
- Reddit: [summary]
- Blogs: [summary]

## Market Opportunities
### Identified Sub-Niches
1. [Sub-niche 1]
2. [Sub-niche 2]

### Trending Keywords
- [Keyword 1], [Keyword 2], ...

## Challenges and Risks
- [Challenge 1]
- [Challenge 2]

IMPORTANT NOTES:
- ALWAYS respect Semantic Scholar rate limit (1 req/sec). Add delays between requests.
- If a tool fails, log the error but continue with available data
- Prioritize quality over quantity (better 5 good sources than 20 mediocre)
- Be objective: report both positive and negative findings
""")

        human_msg = HumanMessage(content=f"""Analyze the following research niche and provide a comprehensive viability assessment:

NICHE: {state['niche']}

Follow all requirements in the system message. Start with academic research, then community activity, then synthesis.""")

        # 5. Invocar agente
        logger.info("niche_analyst_invoking_agent", tools_count=len(tools))
        result = agent.invoke({
            "messages": [system_msg, human_msg]
        })

        # 6. Extraer output
        analysis = result["messages"][-1].content

        logger.info(
            "niche_analyst_completed",
            output_length=len(analysis),
            messages_count=len(result["messages"]),
        )

        # 7. Actualizar estado
        return {
            **state,
            "niche_analysis": analysis,
            "current_agent": "literature_researcher",
            "agent_history": state.get("agent_history", []) + ["niche_analyst"],
            "messages": state.get("messages", []) + result["messages"],
        }

    except Exception as e:
        logger.error(
            "niche_analyst_failed",
            error=str(e),
            error_type=type(e).__name__,
        )

        # Actualizar estado con error
        errors = state.get("errors", [])
        errors.append(f"NicheAnalyst: {str(e)}")

        retry_count = state.get("retry_count", {})
        retry_count["niche_analyst"] = retry_count.get("niche_analyst", 0) + 1

        return {
            **state,
            "errors": errors,
            "retry_count": retry_count,
        }
```

### üîÑ Manejo de Errores y Reintentos

#### Edges Condicionales para Retry Logic

```python
def should_retry_or_continue(state: ResearchState) -> str:
    """
    Decide si reintentar un nodo fallido o continuar/fallar.

    Returns:
        "retry": Vuelve a ejecutar el nodo actual
        "continue": Contin√∫a al siguiente nodo
        "fail": Termina el workflow con error
    """
    errors = state.get("errors", [])
    current_agent = state.get("current_agent", "")
    retry_count = state.get("retry_count", {})

    # Si no hay errores, contin√∫a normalmente
    if len(errors) == 0:
        return "continue"

    # Si ya se reintent√≥ 3 veces, falla
    if retry_count.get(current_agent, 0) >= 3:
        logger.error(
            "max_retries_exceeded",
            agent=current_agent,
            retries=retry_count.get(current_agent),
        )
        return "fail"

    # Reintenta
    logger.warning(
        "retrying_agent",
        agent=current_agent,
        retry_number=retry_count.get(current_agent, 0) + 1,
    )
    return "retry"

# Conectar en el grafo
workflow.add_conditional_edges(
    "niche_analyst",
    should_retry_or_continue,
    {
        "retry": "niche_analyst",
        "continue": "literature_researcher",
        "fail": END,
    }
)
```

### üí∞ Budget Tracking

```python
def check_budget(state: ResearchState) -> str:
    """
    Verifica si se excedi√≥ el presupuesto.

    Returns:
        "continue": Presupuesto OK, contin√∫a
        "stop": Presupuesto excedido, detiene workflow
    """
    total_used = state.get("total_credits_used", 0.0)
    budget_limit = state.get("budget_limit", 10.0)

    if total_used >= budget_limit:
        logger.warning(
            "budget_exceeded",
            total_used=total_used,
            budget_limit=budget_limit,
        )
        return "stop"

    return "continue"

# Conectar despu√©s de cada nodo costoso
workflow.add_conditional_edges(
    "literature_researcher",
    check_budget,
    {
        "continue": "technical_architect",
        "stop": END,
    }
)
```

---

## Visualizaci√≥n de Grafos

### üé® M√©todo 1: Diagrama ASCII (Built-in)

```python
from graphs.research_graph import create_research_graph

graph = create_research_graph()

# Imprimir estructura
graph.get_graph().draw_ascii()
```

**Output**:

```
        +----------------+
        |     START      |
        +----------------+
                *
                *
                *
        +----------------+
        |niche_analyst   |
        +----------------+
                *
                *
                *
        +----------------+
        |literature_...  |
        +----------------+
                ...
```

### üåê M√©todo 2: Diagrama Mermaid (GitHub/Notion)

```python
graph = create_research_graph()

# Generar c√≥digo Mermaid
mermaid_code = graph.get_graph().draw_mermaid()
print(mermaid_code)
```

**Output** (copia y pega en GitHub, Notion, o https://mermaid.live):

```mermaid
graph TD
    START([START]) --> NicheAnalyst[Niche Analyst]
    NicheAnalyst --> LiteratureResearcher[Literature Researcher]
    LiteratureResearcher --> TechnicalArchitect[Technical Architect]
    TechnicalArchitect --> ImplementationSpecialist[Implementation Specialist]
    ImplementationSpecialist --> ContentSynthesizer[Content Synthesizer]
    ContentSynthesizer --> END([END])

    style START fill:#90EE90
    style END fill:#FFB6C1
    style NicheAnalyst fill:#87CEEB
    style LiteratureResearcher fill:#DDA0DD
    style TechnicalArchitect fill:#F0E68C
    style ImplementationSpecialist fill:#FFD700
    style ContentSynthesizer fill:#FFA07A
```

### üñºÔ∏è M√©todo 3: Diagrama PNG (Graphviz)

```python
# Requiere: pip install pygraphviz pillow

from PIL import Image
import io

graph = create_research_graph()

# Generar imagen
img_data = graph.get_graph().draw_mermaid_png()
img = Image.open(io.BytesIO(img_data))

# Guardar
img.save("ara_research_graph.png")

# O mostrar
img.show()
```

### üî¨ M√©todo 4: LangGraph Studio (GUI Oficial)

**LangGraph Studio** es la herramienta oficial de LangChain para desarrollar y debuggear grafos visualmente.

#### Instalaci√≥n:

```bash
pip install langgraph-studio
```

#### Uso:

```bash
# Iniciar servidor
langgraph-studio run graphs/research_graph.py

# Abrir en navegador: http://localhost:8000
```

**Caracter√≠sticas**:

- ‚úÖ Visualizaci√≥n interactiva del grafo
- ‚úÖ Ejecuci√≥n paso a paso (step-by-step debugging)
- ‚úÖ Inspecci√≥n de estado en cada nodo
- ‚úÖ Modificaci√≥n de prompts en vivo
- ‚úÖ Re-ejecuci√≥n de nodos espec√≠ficos
- ‚úÖ Historial de ejecuciones

#### Screenshot Simulado:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LangGraph Studio - ARA Research Pipeline                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ   [START] ‚îÄ‚îÄ‚ñ∫ [Niche Analyst] ‚îÄ‚îÄ‚ñ∫ [Literature...]       ‚îÇ
‚îÇ                    ‚îÇ                                      ‚îÇ
‚îÇ                    ‚îî‚îÄ Status: Running (35%)              ‚îÇ
‚îÇ                    ‚îî‚îÄ Duration: 2m 45s                   ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ   Current State:                                          ‚îÇ
‚îÇ   {                                                       ‚îÇ
‚îÇ     "niche": "Rust WebAssembly...",                      ‚îÇ
‚îÇ     "niche_analysis": "## Viability Score: 78...",      ‚îÇ
‚îÇ     "messages": [15 messages],                           ‚îÇ
‚îÇ     ...                                                   ‚îÇ
‚îÇ   }                                                       ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ   [‚óÑ Prev Node] [‚ñÆ‚ñÆ Pause] [Next Node ‚ñ∫]                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìä M√©todo 5: LangSmith (Observabilidad en Producci√≥n)

**LangSmith** es la plataforma de observabilidad de LangChain, ideal para producci√≥n.

#### Setup:

```bash
# .env
LANGSMITH_API_KEY=ls__...
LANGSMITH_PROJECT=ara-framework
LANGCHAIN_TRACING_V2=true
```

#### C√≥digo:

```python
from langsmith import Client

client = Client()

# Ejecutar con tracing autom√°tico
result = await graph.ainvoke(initial_state)

# Ver en dashboard: https://smith.langchain.com
```

**Caracter√≠sticas**:

- ‚úÖ Tracing completo de todas las llamadas LLM
- ‚úÖ Latencia y costos por nodo
- ‚úÖ Inspecci√≥n de prompts y outputs
- ‚úÖ Comparaci√≥n de ejecuciones
- ‚úÖ An√°lisis de errores y debugging
- ‚úÖ M√©tricas agregadas (P50, P95, P99)

#### Dashboard Simulado:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LangSmith - ARA Framework Traces                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ  Run ID: run_abc123                                       ‚îÇ
‚îÇ  Status: ‚úÖ Success                                        ‚îÇ
‚îÇ  Duration: 65m 23s                                        ‚îÇ
‚îÇ  Cost: $0.18                                              ‚îÇ
‚îÇ  Tokens: 287,456                                          ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  Trace Timeline:                                          ‚îÇ
‚îÇ  ‚îú‚îÄ niche_analyst (7m 12s, $0.00)                        ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ ChatGroq (3 calls, 8,234 tokens)                 ‚îÇ
‚îÇ  ‚îú‚îÄ literature_researcher (22m 45s, $0.12)               ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ ChatGroq (8 calls, 45,678 tokens)                ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ search_tool (156 calls)                           ‚îÇ
‚îÇ  ‚îú‚îÄ technical_architect (11m 03s, $0.03)                 ‚îÇ
‚îÇ  ...                                                      ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  [üì• Export] [üîÑ Rerun] [üìä Analytics]                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üéØ M√©todo 6: Script de Visualizaci√≥n Personalizado

El proyecto incluye `visualize_graph.py`:

```bash
# Ver todos los formatos
python visualize_graph.py

# Solo ASCII
python visualize_graph.py --format ascii

# Solo Mermaid
python visualize_graph.py --format mermaid

# Solo tabla de nodos
python visualize_graph.py --format table
```

---

## Comparaci√≥n con CrewAI

### üìä Tabla Comparativa Detallada

| Aspecto                  | LangGraph                                     | CrewAI                               |
| ------------------------ | --------------------------------------------- | ------------------------------------ |
| **Arquitectura**         | Grafo de estado expl√≠cito                     | Pipeline impl√≠cito con Crews         |
| **Control de Flujo**     | ‚úÖ Explicit (add_edge, add_conditional_edges) | ‚ö†Ô∏è Impl√≠cito (delegaci√≥n autom√°tica) |
| **Estado Compartido**    | ‚úÖ TypedDict centralizado                     | ‚ö†Ô∏è Limitado (context en Crew)        |
| **Checkpointing**        | ‚úÖ Nativo (MemorySaver, RedisSaver)           | ‚ùå No soportado                      |
| **Pause/Resume**         | ‚úÖ S√≠ (con thread_id)                         | ‚ùå No                                |
| **Human-in-the-Loop**    | ‚úÖ F√°cil (pause en cualquier nodo)            | ‚ö†Ô∏è Complejo (custom logic)           |
| **Retry Logic**          | ‚úÖ Condicional edges                          | ‚ö†Ô∏è Requiere custom error handling    |
| **Debugging**            | ‚úÖ Excelente (LangSmith, Studio)              | ‚ö†Ô∏è Dif√≠cil (outputs opacos)          |
| **Testing**              | ‚úÖ F√°cil (mock nodos individuales)            | ‚ö†Ô∏è Dif√≠cil (crews como black box)    |
| **Observabilidad**       | ‚úÖ LangSmith integrado                        | ‚ö†Ô∏è Limited logging                   |
| **Escalabilidad**        | ‚úÖ Alta (usado por Uber, LinkedIn)            | ‚ö†Ô∏è Media (menos maduro)              |
| **Flexibilidad**         | ‚úÖ Total control del flujo                    | ‚ö†Ô∏è Limitado a patterns predefinidos  |
| **Curva de Aprendizaje** | ‚ö†Ô∏è Moderada (conceptos nuevos)                | ‚úÖ Baja (abstracci√≥n simple)         |
| **Documentaci√≥n**        | ‚úÖ Excelente (oficial LangChain)              | ‚ö†Ô∏è Limitada (comunidad m√°s peque√±a)  |
| **Comunidad**            | ‚úÖ Grande (LangChain ecosystem)               | ‚ö†Ô∏è Creciendo                         |
| **Licencia**             | MIT (open source)                             | MIT (open source)                    |
| **Performance**          | ‚úÖ Optimizado (async nativo)                  | ‚ö†Ô∏è Overhead en delegaciones          |
| **Costo**                | Igual (mismo LLM subyacente)                  | Igual (mismo LLM subyacente)         |

### üîÑ Ejemplo de Migraci√≥n: CrewAI ‚Üí LangGraph

#### CrewAI (Antes):

```python
from crewai import Agent, Task, Crew

# Definir agentes
niche_analyst = Agent(
    role="Niche Analyst",
    goal="Analyze market viability",
    backstory="Expert with 10+ years...",
    tools=[scraping_tool, search_tool],
    verbose=True,
)

literature_researcher = Agent(
    role="Literature Researcher",
    goal="Review academic papers",
    backstory="PhD researcher...",
    tools=[search_tool, pdf_tool],
    verbose=True,
)

# Definir tareas
task1 = Task(
    description="Analyze niche: {niche}",
    agent=niche_analyst,
    expected_output="Viability report",
)

task2 = Task(
    description="Review literature based on: {niche_analysis}",
    agent=literature_researcher,
    expected_output="Literature review",
)

# Crear crew
crew = Crew(
    agents=[niche_analyst, literature_researcher],
    tasks=[task1, task2],
    process="sequential",  # o "hierarchical"
    verbose=True,
)

# Ejecutar (bloquea hasta terminar, no hay checkpointing)
result = crew.kickoff(inputs={"niche": "Rust WebAssembly..."})
```

**Problemas**:

- ‚ùå No hay checkpointing (si falla a los 30 min, pierdes todo)
- ‚ùå Estado opaco (no sabes qu√© tiene cada agente)
- ‚ùå Dif√≠cil debugging (outputs mezclados)
- ‚ùå No puedes pausar/reanudar
- ‚ùå Retry logic manual

#### LangGraph (Despu√©s):

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Definir estado
class ResearchState(TypedDict):
    niche: str
    niche_analysis: Optional[str]
    literature_review: Optional[str]
    messages: Annotated[Sequence[BaseMessage], add]

# Definir nodos (funciones simples)
def niche_analyst_node(state: ResearchState) -> ResearchState:
    llm = ChatGroq(model="llama-3.3-70b-versatile")
    tools = [scraping_tool, search_tool]
    agent = create_react_agent(llm, tools)

    result = agent.invoke({
        "messages": [
            SystemMessage(content="You are a niche analyst..."),
            HumanMessage(content=f"Analyze: {state['niche']}")
        ]
    })

    return {
        **state,
        "niche_analysis": result["messages"][-1].content,
    }

def literature_researcher_node(state: ResearchState) -> ResearchState:
    llm = ChatGroq(model="llama-3.3-70b-versatile")
    tools = [search_tool, pdf_tool]
    agent = create_react_agent(llm, tools)

    result = agent.invoke({
        "messages": [
            SystemMessage(content=f"Context: {state['niche_analysis']}\n\nYou are a literature researcher..."),
            HumanMessage(content=f"Review literature for: {state['niche']}")
        ]
    })

    return {
        **state,
        "literature_review": result["messages"][-1].content,
    }

# Construir grafo
workflow = StateGraph(ResearchState)
workflow.add_node("niche_analyst", niche_analyst_node)
workflow.add_node("literature_researcher", literature_researcher_node)

workflow.set_entry_point("niche_analyst")
workflow.add_edge("niche_analyst", "literature_researcher")
workflow.add_edge("literature_researcher", END)

# Compilar con checkpointing
graph = workflow.compile(checkpointer=MemorySaver())

# Ejecutar con pause/resume capability
config = {"configurable": {"thread_id": "research-001"}}
result = await graph.ainvoke(
    {"niche": "Rust WebAssembly...", "messages": []},
    config=config,
)
```

**Ventajas**:

- ‚úÖ Checkpointing autom√°tico (recovery gratis)
- ‚úÖ Estado expl√≠cito y type-safe
- ‚úÖ F√°cil debugging (inspecciona estado en cualquier punto)
- ‚úÖ Pause/resume con thread_id
- ‚úÖ Retry logic con conditional edges

### üìà Por Qu√© Elegimos LangGraph

**Razones de la migraci√≥n**:

1. **Checkpointing cr√≠tico**: Con an√°lisis de 60-75 min, perder todo por un fallo es inaceptable
2. **Control expl√≠cito**: Necesitamos saber exactamente qu√© hace cada agente y cu√°ndo
3. **Debugging**: CrewAI era una caja negra, LangGraph permite inspeccionar cada paso
4. **Escalabilidad**: LangGraph est√° respaldado por LangChain (usado por empresas Fortune 500)
5. **Observabilidad**: Integraci√≥n nativa con LangSmith para producci√≥n
6. **Flexibilidad**: Agregar conditional edges, loops, human-in-the-loop es trivial

**Resultado**:

- ‚úÖ 0 p√©rdidas de datos por fallos (checkpointing)
- ‚úÖ 50% menos tiempo de debugging (LangSmith traces)
- ‚úÖ 100% reproducibilidad (estado serializable)
- ‚úÖ Ready para producci√≥n (usado por LinkedIn, Uber, etc.)

---

## Casos de Uso Avanzados

### üîÅ 1. Loops y Ciclos

**Escenario**: Refinar un an√°lisis hasta que cumpla criterios de calidad.

```python
def quality_check(state: ResearchState) -> str:
    """Verifica si el an√°lisis cumple est√°ndares de calidad."""
    analysis = state.get("niche_analysis", "")

    # Criterios de calidad
    has_viability_score = "Viability Score:" in analysis
    has_papers = analysis.count("paper") >= 5
    is_long_enough = len(analysis) >= 2000

    if has_viability_score and has_papers and is_long_enough:
        return "pass"

    # Si ya se intent√≥ 3 veces, contin√∫a igual
    if state.get("retry_count", {}).get("niche_analyst", 0) >= 3:
        return "pass"

    return "retry"

# Configurar loop
workflow.add_conditional_edges(
    "niche_analyst",
    quality_check,
    {
        "pass": "literature_researcher",  # Contin√∫a
        "retry": "niche_analyst",         # Vuelve a ejecutar
    }
)
```

### ü§ù 2. Human-in-the-Loop

**Escenario**: Pausar despu√©s del an√°lisis de nicho para que el usuario apruebe antes de continuar.

```python
def needs_human_approval(state: ResearchState) -> str:
    """Pausa para esperar aprobaci√≥n humana."""
    return "pending"

# Agregar nodo de aprobaci√≥n humana
workflow.add_node("human_approval", lambda state: state)  # No-op

workflow.add_conditional_edges(
    "niche_analyst",
    needs_human_approval,
    {
        "pending": "human_approval",
    }
)

workflow.add_edge("human_approval", "literature_researcher")

# Ejecutar
config = {"configurable": {"thread_id": "research-001"}}
result = await graph.ainvoke(initial_state, config=config)

# Estado pausado en "human_approval"
print(result["niche_analysis"])  # Usuario revisa

# Usuario aprueba y reanuda
result = await graph.ainvoke(
    {"approved": True},  # Actualiza estado
    config=config,        # Mismo thread_id
)
```

### üîÄ 3. Paralelizaci√≥n de Nodos

**Escenario**: Ejecutar m√∫ltiples agentes en paralelo (ej: buscar en Semantic Scholar y GitHub simult√°neamente).

```python
from langgraph.graph import ParallelNode

# Nodos paralelos
def search_papers_node(state: ResearchState) -> ResearchState:
    # Buscar papers
    return {**state, "papers": papers}

def search_github_node(state: ResearchState) -> ResearchState:
    # Buscar repos
    return {**state, "repos": repos}

# Agregar como paralelo
workflow.add_node(
    "parallel_search",
    ParallelNode([search_papers_node, search_github_node])
)

workflow.add_edge("niche_analyst", "parallel_search")
workflow.add_edge("parallel_search", "literature_researcher")
```

### üéõÔ∏è 4. Routing Din√°mico

**Escenario**: Decidir qu√© agente ejecutar bas√°ndose en el nicho.

```python
def route_by_niche(state: ResearchState) -> str:
    """Enruta a diferentes agentes seg√∫n el tipo de nicho."""
    niche = state["niche"].lower()

    if "machine learning" in niche or "ai" in niche:
        return "ml_specialist"
    elif "web" in niche or "frontend" in niche:
        return "web_specialist"
    else:
        return "general_researcher"

# Agregar nodos especializados
workflow.add_node("ml_specialist", ml_specialist_node)
workflow.add_node("web_specialist", web_specialist_node)
workflow.add_node("general_researcher", general_researcher_node)

# Routing din√°mico
workflow.add_conditional_edges(
    "niche_analyst",
    route_by_niche,
    {
        "ml_specialist": "ml_specialist",
        "web_specialist": "web_specialist",
        "general_researcher": "general_researcher",
    }
)
```

### üîê 5. Budget y Rate Limiting

**Escenario**: Detener el workflow si se excede el presupuesto o rate limits.

```python
class ResearchState(TypedDict):
    ...
    total_api_calls: int
    rate_limit_per_minute: int
    last_api_call_time: Optional[str]

def check_rate_limit(state: ResearchState) -> str:
    """Verifica rate limits antes de continuar."""
    now = datetime.now(timezone.utc)
    last_call = datetime.fromisoformat(state.get("last_api_call_time", now.isoformat()))

    # Semantic Scholar: max 1 req/sec
    if (now - last_call).total_seconds() < 1.0:
        return "wait"

    # Budget check
    total_cost = state.get("total_credits_used", 0.0)
    if total_cost >= state.get("budget_limit", 10.0):
        return "stop"

    return "continue"

workflow.add_conditional_edges(
    "literature_researcher",
    check_rate_limit,
    {
        "wait": "wait_node",  # Pausa 1 segundo
        "continue": "technical_architect",
        "stop": END,
    }
)
```

### üìä 6. A/B Testing de Prompts

**Escenario**: Probar 2 versiones de prompts y comparar resultados.

```python
import random

def niche_analyst_v1(state: ResearchState) -> ResearchState:
    """Versi√≥n 1: Prompt original"""
    # ...

def niche_analyst_v2(state: ResearchState) -> ResearchState:
    """Versi√≥n 2: Prompt optimizado"""
    # ...

def ab_test_router(state: ResearchState) -> str:
    """Selecciona versi√≥n aleatoriamente."""
    return "v1" if random.random() < 0.5 else "v2"

workflow.add_node("niche_analyst_v1", niche_analyst_v1)
workflow.add_node("niche_analyst_v2", niche_analyst_v2)

workflow.add_conditional_edges(
    START,
    ab_test_router,
    {
        "v1": "niche_analyst_v1",
        "v2": "niche_analyst_v2",
    }
)

# Ambas versiones convergen en el mismo siguiente nodo
workflow.add_edge("niche_analyst_v1", "literature_researcher")
workflow.add_edge("niche_analyst_v2", "literature_researcher")
```

### üß™ 7. Testing y Mocking

**Escenario**: Testear el grafo sin llamar LLMs reales.

```python
# tests/test_langgraph.py

import pytest
from unittest.mock import Mock, patch
from graphs.research_graph import niche_analyst_node, ResearchState

@pytest.mark.asyncio
async def test_niche_analyst_node():
    """Test del nodo sin llamar API real."""

    # Mock del agente
    mock_result = {
        "messages": [
            Mock(content="## Viability Score: 85\n\nExcellent niche...")
        ]
    }

    with patch("graphs.research_graph.create_react_agent") as mock_agent:
        mock_agent.return_value.invoke.return_value = mock_result

        # Estado inicial
        state: ResearchState = {
            "niche": "Rust WebAssembly",
            "messages": [],
            "agent_history": [],
            "errors": [],
        }

        # Ejecutar nodo
        result = niche_analyst_node(state)

        # Assertions
        assert "niche_analysis" in result
        assert "Viability Score: 85" in result["niche_analysis"]
        assert "niche_analyst" in result["agent_history"]
        assert len(result["messages"]) > 0

@pytest.mark.asyncio
async def test_full_graph_with_mocks():
    """Test del grafo completo con nodos mockeados."""

    from graphs.research_graph import create_research_graph

    # Mockear todos los nodos
    with patch("graphs.research_graph.niche_analyst_node") as mock_n1, \
         patch("graphs.research_graph.literature_researcher_node") as mock_n2:

        mock_n1.side_effect = lambda s: {**s, "niche_analysis": "Mock analysis"}
        mock_n2.side_effect = lambda s: {**s, "literature_review": "Mock review"}

        graph = create_research_graph(enable_checkpointing=False)

        result = await graph.ainvoke({
            "niche": "Test niche",
            "messages": [],
        })

        assert result["niche_analysis"] == "Mock analysis"
        assert result["literature_review"] == "Mock review"
```

---

## üéì Recursos Adicionales

### üìñ Documentaci√≥n Oficial

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **LangChain Docs**: https://python.langchain.com/docs/
- **LangSmith**: https://docs.smith.langchain.com/

### üé• Tutoriales y Videos

- **LangGraph Tutorial** (LangChain): https://www.youtube.com/watch?v=aWS8VMLwlYM
- **Building Multi-Agent Systems**: https://www.youtube.com/watch?v=hvAPnpSfSGo
- **LangGraph Deep Dive**: https://www.youtube.com/watch?v=3T9JH3SqSF4

### üìö Ejemplos de C√≥digo

- **LangGraph Examples** (GitHub): https://github.com/langchain-ai/langgraph/tree/main/examples
- **Multi-Agent Collaboration**: https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi_agent_collaboration.ipynb
- **Human-in-the-Loop**: https://github.com/langchain-ai/langgraph/blob/main/examples/human-in-the-loop.ipynb

### üõ†Ô∏è Herramientas Complementarias

- **LangGraph Studio**: GUI para desarrollo y debugging
- **LangSmith**: Observabilidad y monitoring en producci√≥n
- **LangServe**: Deploy de grafos como APIs REST
- **LangFuse**: Alternativa open-source a LangSmith

### üí¨ Comunidad

- **Discord**: https://discord.gg/langchain
- **Twitter**: @LangChainAI
- **GitHub Discussions**: https://github.com/langchain-ai/langgraph/discussions

---

## üéâ Conclusi√≥n

**LangGraph** es el framework ideal para construir aplicaciones multi-agente complejas y production-ready. Sus caracter√≠sticas de checkpointing, estado centralizado, y control expl√≠cito lo hacen superior a alternativas como CrewAI para proyectos serios.

### ‚úÖ Checklist de Migraci√≥n a LangGraph

- [x] Definir estado centralizado (TypedDict)
- [x] Convertir agentes a funciones de nodo
- [x] Crear StateGraph y agregar nodos
- [x] Conectar nodos con edges (secuenciales y condicionales)
- [x] Implementar checkpointing (MemorySaver o Redis)
- [x] Agregar error handling y retry logic
- [x] Configurar observabilidad (LangSmith)
- [x] Escribir tests con mocks
- [x] Documentar flujo y visualizar grafo

### üöÄ Pr√≥ximos Pasos

1. **Explorar LangGraph Studio**: Instala y visualiza el grafo interactivamente
2. **Implementar conditional edges**: Agrega retry logic y budget checks
3. **Configurar LangSmith**: Habilita tracing en producci√≥n
4. **Optimizar prompts**: Usa A/B testing para mejorar outputs
5. **Escalar a producci√≥n**: Deploy con LangServe o FastAPI

---

**¬øPreguntas?** Revisa la documentaci√≥n oficial o abre un issue en GitHub: https://github.com/langchain-ai/langgraph/issues

**Happy Graph Building! üé®üìäüöÄ**
